{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfd17a-cab9-4836-b276-57dc7cebca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################## gemini  ##################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c96ac80-ac6f-48ec-a75d-b2a56ce7dfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: evaluate in c:\\users\\hp\\anaconda3\\lib\\site-packages (0.4.6)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (4.1.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (0.34.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\hp\\anaconda3\\lib\\site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (21.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.10.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests>=2.19.0->evaluate) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pandas->evaluate) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
      "Found existing installation: pyarrow 21.0.0\n",
      "Uninstalling pyarrow-21.0.0:\n",
      "  Successfully uninstalled pyarrow-21.0.0\n",
      "Collecting pyarrow\n",
      "  Using cached pyarrow-21.0.0-cp312-cp312-win_amd64.whl.metadata (3.4 kB)\n",
      "Using cached pyarrow-21.0.0-cp312-cp312-win_amd64.whl (26.2 MB)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-21.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate\n",
    "!pip uninstall pyarrow -y\n",
    "!pip install pyarrow --upgrade\n",
    "!conda install -c conda-forge pyarrow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae459f1d-cd74-4451-8445-47abcef182e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Combining HTML Files and Extracting Text ---\n",
      "Extracted text data saved to: combined_ticket_data.txt\n",
      "\n",
      "--- Step 2: Generating and Executing Advanced Prompts ---\n",
      "\n",
      "======================================================================\n",
      "EXECUTING PROMPT TYPE: A. Zero-Shot\n",
      "----------------------------------------------------------------------\n",
      "PROMPT SENT (Snippet):\n",
      "Classify the following ticket into one of these categories: [Hardware, Software, Access Request, Net...\n",
      "\n",
      "GEMINI RESPONSE (Full):\n",
      "Classification: **Network**\n",
      "Error during ROUGE calculation: 'numpy.float64' object has no attribute 'mid'\n",
      "Error during Perplexity calculation (returning inf): Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\n",
      "\n",
      "**********************************************************************\n",
      "EVALUATION RESULTS for A. Zero-Shot:\n",
      "  -> Reference Answer (Classification): Network\n",
      "  -> Model's Cleaned Classification:    Network\n",
      "  -> ROUGE-L F1 Score:                0.0000 (Measures Content Overlap, Max 1.0000)\n",
      "  -> Perplexity (PPL) Score:          inf (Measures Fluency/Confidence, Lower is Better)\n",
      "**********************************************************************\n",
      "\n",
      "======================================================================\n",
      "EXECUTING PROMPT TYPE: B. Few-Shot\n",
      "----------------------------------------------------------------------\n",
      "PROMPT SENT (Snippet):\n",
      "Here are some examples of ticket classification:  Example 1: TICKET: \"My monitor has a thin vertical...\n",
      "\n",
      "GEMINI RESPONSE (Full):\n",
      "Classification: Network\n",
      "Error during ROUGE calculation: 'numpy.float64' object has no attribute 'mid'\n",
      "Error during Perplexity calculation (returning inf): Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\n",
      "\n",
      "**********************************************************************\n",
      "EVALUATION RESULTS for B. Few-Shot:\n",
      "  -> Reference Answer (Classification): Network\n",
      "  -> Model's Cleaned Classification:    Network\n",
      "  -> ROUGE-L F1 Score:                0.0000 (Measures Content Overlap, Max 1.0000)\n",
      "  -> Perplexity (PPL) Score:          inf (Measures Fluency/Confidence, Lower is Better)\n",
      "**********************************************************************\n",
      "\n",
      "======================================================================\n",
      "EXECUTING PROMPT TYPE: C. Chain-of-Thought (COT)\n",
      "----------------------------------------------------------------------\n",
      "PROMPT SENT (Snippet):\n",
      "**Instructions:** First, analyze the TICKET and describe the core issue and its impact. Second, dete...\n",
      "\n",
      "GEMINI RESPONSE (Full):\n",
      "**Reasoning Process:**\n",
      "\n",
      "1.  **Analyze the TICKET:** \"The VPN connection keeps dropping every 10 minutes. I can't work.\"\n",
      "    *   **Core Issue:** The Virtual Private Network (VPN) connection is unstable and frequently disconnecting.\n",
      "    *   **Impact:** The user is unable to perform their job duties, indicating a significant productivity loss and a critical impediment to work.\n",
      "\n",
      "2.  **Determine Classification:**\n",
      "    *   **Hardware:** Not directly indicated. While a network card or router could be faulty, the symptom \"VPN connection keeps dropping\" points more broadly to the connection itself rather than a specific piece of physical equipment.\n",
      "    *   **Software:** While the VPN client is software, the issue is with the *connection* it establishes. A dropping connection is fundamentally a network stability issue, not necessarily a bug in the software application itself (though a misconfigured client could contribute). The problem is the integrity of the tunnel.\n",
      "    *   **Access Request:** The user *has* access, but it's not stable. This is not a request for new access.\n",
      "    *   **Network:** VPNs create secure network connections. A connection that \"keeps dropping\" is a classic symptom of a network stability issue. This could involve the user's local network, their ISP, the company's VPN server infrastructure, firewall rules, or general internet routing stability affecting the VPN tunnel. The core problem lies in the reliability of the network link provided by the VPN.\n",
      "\n",
      "3.  **Conclusion:** The problem is directly related to the stability and functionality of a network connection (VPN).\n",
      "\n",
      "Network\n",
      "Error during ROUGE calculation: 'numpy.float64' object has no attribute 'mid'\n",
      "Error during Perplexity calculation (returning inf): Asking to pad but the tokenizer does not have a padding token. Please select a token to use as `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via `tokenizer.add_special_tokens({'pad_token': '[PAD]'})`.\n",
      "\n",
      "**********************************************************************\n",
      "EVALUATION RESULTS for C. Chain-of-Thought (COT):\n",
      "  -> Reference Answer (Classification): Network\n",
      "  -> Model's Cleaned Classification:    Network\n",
      "  -> ROUGE-L F1 Score:                0.0000 (Measures Content Overlap, Max 1.0000)\n",
      "  -> Perplexity (PPL) Score:          inf (Measures Fluency/Confidence, Lower is Better)\n",
      "**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import evaluate # For ROUGE and Perplexity evaluation\n",
    "from typing import List, Dict, Tuple\n",
    "from evaluate import load as evaluate_load # For loading metrics\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# ⚠️ REPLACE WITH YOUR ACTUAL API KEY ⚠️\n",
    "API_KEY = \"AIzaSyDSlwTLHc2FDJUZ5w5-S_ZltEFWBmvAUTQ\"  # Replace with your actual key\n",
    "try:\n",
    "    genai.configure(api_key=API_KEY)\n",
    "except Exception as e:\n",
    "    # A simple exit is better than proceeding with a bad key\n",
    "    print(f\"Error configuring API: {e}. Please check your API key.\")\n",
    "    # You might need to exit the script here in a real application\n",
    "\n",
    "MODEL_NAME = \"gemini-2.5-flash\"\n",
    "model = genai.GenerativeModel(MODEL_NAME)\n",
    "\n",
    "OUTPUT_HTML_FILE = \"combined_output.html\"\n",
    "OUTPUT_TEXT_FILE = \"combined_ticket_data.txt\"\n",
    "\n",
    "# --- Dummy Data Setup (for a runnable example) ---\n",
    "html_doc = \"\"\"\n",
    "<html><body>\n",
    "<p>Ticket 1: My laptop's screen is flickering after the new software update. It is critical!</p>\n",
    "<p>Ticket 2: I need access to the Marketing share drive please. This is low priority.</p>\n",
    "<p>Ticket 3: The VPN connection keeps dropping every 10 minutes. I can't work.</p>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "combined_soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# --- 2. HTML COMBINATION & TEXT EXTRACTION ---\n",
    "\n",
    "print(\"--- Step 1: Combining HTML Files and Extracting Text ---\")\n",
    "\n",
    "# Save the combined HTML (from your original request)\n",
    "with open(OUTPUT_HTML_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(combined_soup.prettify())\n",
    "\n",
    "# Extract all relevant text from the combined document\n",
    "all_ticket_text = combined_soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "# Save the extracted text to a file (optional)\n",
    "with open(OUTPUT_TEXT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(all_ticket_text)\n",
    "print(f\"Extracted text data saved to: {OUTPUT_TEXT_FILE}\")\n",
    "\n",
    "# --- 3. PROMPT GENERATION AND EXECUTION ---\n",
    "\n",
    "print(\"\\n--- Step 2: Generating and Executing Advanced Prompts ---\")\n",
    "\n",
    "# The specific ticket we will classify for the Few-Shot and COT examples\n",
    "TICKET_TO_CLASSIFY = \"The VPN connection keeps dropping every 10 minutes. I can't work.\"\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# GOLD STANDARD REFERENCE (Ground Truth) for ROUGE Evaluation\n",
    "# --------------------------------------------------------------------------\n",
    "REFERENCE_ANSWER: List[str] = [\"Network\"]\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# PROMPT TYPE A: ZERO-SHOT PROMPTING \n",
    "# --------------------------------------------------------------------------\n",
    "zero_shot_prompt = f\"\"\"\n",
    "Classify the following ticket into one of these categories: [Hardware, Software, Access Request, Network].\n",
    "TICKET: \"{TICKET_TO_CLASSIFY}\"\n",
    "Classification:\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# PROMPT TYPE B: FEW-SHOT PROMPTING \n",
    "# --------------------------------------------------------------------------\n",
    "few_shot_prompt = f\"\"\"\n",
    "Here are some examples of ticket classification:\n",
    "\n",
    "Example 1:\n",
    "TICKET: \"My monitor has a thin vertical green line on the right side.\"\n",
    "Classification: Hardware\n",
    "\n",
    "Example 2:\n",
    "TICKET: \"Please add me to the 'External Partners' distribution list.\"\n",
    "Classification: Access Request\n",
    "\n",
    "Now, classify this new ticket:\n",
    "TICKET: \"{TICKET_TO_CLASSIFY}\"\n",
    "Classification:\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# PROMPT TYPE C: CHAIN-OF-THOUGHT (COT) PROMPTING \n",
    "# --------------------------------------------------------------------------\n",
    "cot_prompt = f\"\"\"\n",
    "**Instructions:** First, analyze the TICKET and describe the core issue and its impact. Second, determine the correct classification from the list [Hardware, Software, Access Request, Network]. Third, output ONLY the final Classification.\n",
    "\n",
    "TICKET: \"{TICKET_TO_CLASSIFY}\"\n",
    "\n",
    "**Reasoning Process:**\n",
    "\"\"\"\n",
    "\n",
    "# --- Initialize Metrics ---\n",
    "rouge = evaluate_load(\"rouge\")\n",
    "# Load the Perplexity metric using 'gpt2' as the external evaluation model\n",
    "# We set keep_in_memory=True to potentially avoid re-loading the model in a loop,\n",
    "# though the warnings suggest the issue is primarily environmental.\n",
    "perplexity_metric = evaluate_load(\"perplexity\", module_type=\"metric\", keep_in_memory=True) \n",
    "\n",
    "# --- Define Evaluation Functions ---\n",
    "def calculate_rouge_l(candidate_text: str, reference_texts: List[str]) -> float:\n",
    "    \"\"\"Calculates the ROUGE-L F1 score.\"\"\"\n",
    "    try:\n",
    "        # Check if candidate_text is effectively empty after stripping\n",
    "        if not candidate_text.strip():\n",
    "             return 0.0\n",
    "             \n",
    "        results = rouge.compute(\n",
    "            predictions=[candidate_text],\n",
    "            references=reference_texts,\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        return results[\"rougeL\"].mid.fmeasure\n",
    "    except Exception as e:\n",
    "        print(f\"Error during ROUGE calculation: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def calculate_perplexity(candidate_text: str) -> float:\n",
    "    \"\"\"Calculates the Perplexity score using a pre-trained model (gpt2).\"\"\"\n",
    "    # Only calculate if the text is not empty\n",
    "    if not candidate_text.strip():\n",
    "        return float('inf')\n",
    "        \n",
    "    try:\n",
    "        # PPL expects a list of strings\n",
    "        # Setting batch_size=1 helps with simple, single-sequence inputs\n",
    "        results = perplexity_metric.compute(\n",
    "            model_id='gpt2', \n",
    "            predictions=[candidate_text],\n",
    "            batch_size=1\n",
    "        )\n",
    "        # The result is a list of PPL values; we take the first one.\n",
    "        return results['perplexities'][0]\n",
    "    except Exception as e:\n",
    "        # This may occur if the text is empty or the model fails to load\n",
    "        print(f\"Error during Perplexity calculation (returning inf): {e}\")\n",
    "        return float('inf') # Return infinity for a failed calculation\n",
    "\n",
    "# --- Execute Prompts and Evaluate ---\n",
    "prompts_list: List[Tuple[str, str]] = [\n",
    "    (\"A. Zero-Shot\", zero_shot_prompt),\n",
    "    (\"B. Few-Shot\", few_shot_prompt),\n",
    "    (\"C. Chain-of-Thought (COT)\", cot_prompt)\n",
    "]\n",
    "\n",
    "for name, prompt in prompts_list:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXECUTING PROMPT TYPE: {name}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    candidate_response = \"\"\n",
    "    clean_response = \"\"\n",
    "    try:\n",
    "        # 1. Generate Content\n",
    "        response = model.generate_content(prompt)\n",
    "        candidate_response = response.text.strip()\n",
    "        \n",
    "        print(\"PROMPT SENT (Snippet):\")\n",
    "        print(prompt.strip().replace('\\n', ' ')[:100] + \"...\") \n",
    "        print(\"\\nGEMINI RESPONSE (Full):\")\n",
    "        print(candidate_response)\n",
    "        \n",
    "        # 2. Clean Response for ROUGE (Classification Only)\n",
    "        # This logic is designed to extract the final category string\n",
    "        if \"Chain-of-Thought\" in name:\n",
    "            # For COT, search for the last non-empty line as the final classification\n",
    "            lines = [line.strip() for line in candidate_response.split('\\n') if line.strip()]\n",
    "            clean_response = lines[-1].replace(\"Classification:\", \"\").replace(\"Classification\", \"\").strip() if lines else candidate_response\n",
    "        else:\n",
    "            # For Zero/Few-Shot, clean the whole response\n",
    "            clean_response = candidate_response.replace(\"Classification:\", \"\").strip()\n",
    "        \n",
    "        # Strip potential markdown bolding characters\n",
    "        clean_response = clean_response.strip('*')\n",
    "\n",
    "        # 3. EVALUATION\n",
    "        rouge_l_f1 = calculate_rouge_l(clean_response, REFERENCE_ANSWER)\n",
    "        # Use the FULL response for PPL to evaluate overall fluency/coherence\n",
    "        perplexity_score = calculate_perplexity(candidate_response) \n",
    "        \n",
    "        # 4. Print Results\n",
    "        print(f\"\\n{'*'*70}\")\n",
    "        print(f\"EVALUATION RESULTS for {name}:\")\n",
    "        print(f\"  -> Reference Answer (Classification): {REFERENCE_ANSWER[0]}\")\n",
    "        print(f\"  -> Model's Cleaned Classification:    {clean_response}\")\n",
    "        print(f\"  -> ROUGE-L F1 Score:                {rouge_l_f1:.4f} (Measures Content Overlap, Max 1.0000)\")\n",
    "        print(f\"  -> Perplexity (PPL) Score:          {perplexity_score:.2f} (Measures Fluency/Confidence, Lower is Better)\")\n",
    "        print(f\"{'*'*70}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred (Error Type: {type(e).__name__}): {e}\")\n",
    "        print(\"Skipping evaluation for this prompt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218b2e1a-4856-45a6-b70a-237457f1f713",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "##################################OpenAIcode#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9ff16bb-0db2-44fe-a944-0ffa06f54aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Combining HTML Files and Extracting Text ---\n",
      "Extracted text data saved to: combined_ticket_data.txt\n",
      "\n",
      "--- Step 2: Generating and Executing Advanced Prompts ---\n",
      "\n",
      "======================================================================\n",
      "EXECUTING PROMPT TYPE: A. Zero-Shot\n",
      "----------------------------------------------------------------------\n",
      "An API or execution error occurred (Error Type: RateLimitError): Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Skipping evaluation for this prompt.\n",
      "\n",
      "======================================================================\n",
      "EXECUTING PROMPT TYPE: B. Few-Shot\n",
      "----------------------------------------------------------------------\n",
      "An API or execution error occurred (Error Type: RateLimitError): Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Skipping evaluation for this prompt.\n",
      "\n",
      "======================================================================\n",
      "EXECUTING PROMPT TYPE: C. Chain-of-Thought (COT)\n",
      "----------------------------------------------------------------------\n",
      "An API or execution error occurred (Error Type: RateLimitError): Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}\n",
      "Skipping evaluation for this prompt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI # Import the OpenAI library\n",
    "from bs4 import BeautifulSoup\n",
    "import evaluate # For ROUGE and Perplexity evaluation\n",
    "from typing import List, Dict, Tuple\n",
    "from evaluate import load as evaluate_load\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# IMPORTANT: It is best practice to load the API key from an environment variable.\n",
    "# Replace the placeholder below with your actual key if not using an env var, or\n",
    "# set the OPENAI_API_KEY environment variable.\n",
    "# For this script to run successfully, you MUST replace the placeholder.\n",
    "API_KEY = \"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\"\n",
    "if API_KEY == \"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\":\n",
    "    # Attempt to load from environment variable as a fallback\n",
    "    API_KEY = os.environ.get(\"OPENAI_API_KEY\", API_KEY) \n",
    "\n",
    "try:\n",
    "    # Initialize the OpenAI Client\n",
    "    client = OpenAI(api_key=API_KEY)\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing OpenAI client: {e}\")\n",
    "    # Exit or raise error if client initialization fails in a real application\n",
    "\n",
    "# Using a comparable OpenAI model for the task\n",
    "MODEL_NAME = \"gpt-3.5-turbo\" \n",
    "\n",
    "OUTPUT_HTML_FILE = \"combined_output.html\"\n",
    "OUTPUT_TEXT_FILE = \"combined_ticket_data.txt\"\n",
    "\n",
    "# --- Dummy Data Setup (for a runnable example) ---\n",
    "html_doc = \"\"\"\n",
    "<html><body>\n",
    "<p>Ticket 1: My laptop's screen is flickering after the new software update. It is critical!</p>\n",
    "<p>Ticket 2: I need access to the Marketing share drive please. This is low priority.</p>\n",
    "<p>Ticket 3: The VPN connection keeps dropping every 10 minutes. I can't work.</p>\n",
    "</body></html>\n",
    "\"\"\"\n",
    "combined_soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "# --- 2. HTML COMBINATION & TEXT EXTRACTION (Same as previous script) ---\n",
    "\n",
    "print(\"--- Step 1: Combining HTML Files and Extracting Text ---\")\n",
    "\n",
    "with open(OUTPUT_HTML_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(combined_soup.prettify())\n",
    "\n",
    "all_ticket_text = combined_soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "with open(OUTPUT_TEXT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(all_ticket_text)\n",
    "print(f\"Extracted text data saved to: {OUTPUT_TEXT_FILE}\")\n",
    "\n",
    "# --- 3. PROMPT GENERATION AND EXECUTION ---\n",
    "\n",
    "print(\"\\n--- Step 2: Generating and Executing Advanced Prompts ---\")\n",
    "\n",
    "TICKET_TO_CLASSIFY = \"The VPN connection keeps dropping every 10 minutes. I can't work.\"\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# GOLD STANDARD REFERENCE (Ground Truth) for ROUGE Evaluation\n",
    "# --------------------------------------------------------------------------\n",
    "REFERENCE_ANSWER: List[str] = [\"Network\"]\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# PROMPT TYPE A: ZERO-SHOT PROMPTING 🎯\n",
    "# --------------------------------------------------------------------------\n",
    "zero_shot_prompt = f\"\"\"\n",
    "Classify the following ticket into one of these categories: [Hardware, Software, Access Request, Network].\n",
    "TICKET: \"{TICKET_TO_CLASSIFY}\"\n",
    "Classification:\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# PROMPT TYPE B: FEW-SHOT PROMPTING 📝\n",
    "# --------------------------------------------------------------------------\n",
    "few_shot_prompt = f\"\"\"\n",
    "Here are some examples of ticket classification:\n",
    "\n",
    "Example 1:\n",
    "TICKET: \"My monitor has a thin vertical green line on the right side.\"\n",
    "Classification: Hardware\n",
    "\n",
    "Example 2:\n",
    "TICKET: \"Please add me to the 'External Partners' distribution list.\"\n",
    "Classification: Access Request\n",
    "\n",
    "Now, classify this new ticket:\n",
    "TICKET: \"{TICKET_TO_CLASSIFY}\"\n",
    "Classification:\n",
    "\"\"\"\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# PROMPT TYPE C: CHAIN-OF-THOUGHT (COT) PROMPTING 🤔\n",
    "# --------------------------------------------------------------------------\n",
    "cot_prompt = f\"\"\"\n",
    "**Instructions:** First, analyze the TICKET and describe the core issue and its impact. Second, determine the correct classification from the list [Hardware, Software, Access Request, Network]. Third, output ONLY the final Classification.\n",
    "\n",
    "TICKET: \"{TICKET_TO_CLASSIFY}\"\n",
    "\n",
    "**Reasoning Process:**\n",
    "\"\"\"\n",
    "\n",
    "# --- Initialize Metrics (ROUGE and Perplexity) ---\n",
    "# We still use the 'evaluate' library for metrics, as it is language model agnostic\n",
    "rouge = evaluate_load(\"rouge\")\n",
    "perplexity_metric = evaluate_load(\"perplexity\", module_type=\"metric\", keep_in_memory=True) \n",
    "\n",
    "# --- Define Evaluation Functions (Identical to previous script) ---\n",
    "def calculate_rouge_l(candidate_text: str, reference_texts: List[str]) -> float:\n",
    "    \"\"\"Calculates the ROUGE-L F1 score.\"\"\"\n",
    "    try:\n",
    "        if not candidate_text.strip():\n",
    "             return 0.0\n",
    "             \n",
    "        results = rouge.compute(\n",
    "            predictions=[candidate_text],\n",
    "            references=reference_texts,\n",
    "            use_stemmer=True\n",
    "        )\n",
    "        # ROUGE-L F1 score is typically found under 'rougeL' mid fmeasure\n",
    "        return results[\"rougeL\"].mid.fmeasure\n",
    "    except Exception as e:\n",
    "        print(f\"Error during ROUGE calculation: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def calculate_perplexity(candidate_text: str) -> float:\n",
    "    \"\"\"Calculates the Perplexity score using a pre-trained model (gpt2).\"\"\"\n",
    "    if not candidate_text.strip():\n",
    "        return float('inf')\n",
    "        \n",
    "    try:\n",
    "        # PPL calculation uses GPT-2 for evaluation, not the main classification model (GPT-3.5-turbo)\n",
    "        results = perplexity_metric.compute(\n",
    "            model_id='gpt2', \n",
    "            predictions=[candidate_text],\n",
    "            batch_size=1\n",
    "        )\n",
    "        return results['perplexities'][0]\n",
    "    except Exception as e:\n",
    "        # The warnings seen previously might lead here if the model fails to load\n",
    "        print(f\"Error during Perplexity calculation (returning inf): {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "# --- Execute Prompts and Evaluate ---\n",
    "prompts_list: List[Tuple[str, str]] = [\n",
    "    (\"A. Zero-Shot\", zero_shot_prompt),\n",
    "    (\"B. Few-Shot\", few_shot_prompt),\n",
    "    (\"C. Chain-of-Thought (COT)\", cot_prompt)\n",
    "]\n",
    "\n",
    "for name, prompt in prompts_list:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXECUTING PROMPT TYPE: {name}\")\n",
    "    print(f\"{'-'*70}\")\n",
    "    \n",
    "    candidate_response = \"\"\n",
    "    clean_response = \"\"\n",
    "    \n",
    "    if API_KEY == \"YOUR_OPENAI_API_KEY_HERE\":\n",
    "        print(\"!! WARNING: OpenAI API Key not configured. Skipping API call. !!\")\n",
    "        print(\"Please set your OPENAI_API_KEY environment variable or replace the placeholder.\")\n",
    "        continue\n",
    "        \n",
    "    try:\n",
    "        # 1. Generate Content using OpenAI API\n",
    "        # Using a low temperature for deterministic classification tasks\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.0\n",
    "        )\n",
    "        candidate_response = response.choices[0].message.content.strip()\n",
    "        \n",
    "        print(\"PROMPT SENT (Snippet):\")\n",
    "        print(prompt.strip().replace('\\n', ' ')[:100] + \"...\") \n",
    "        print(\"\\nOPENAI RESPONSE (Full):\")\n",
    "        print(candidate_response)\n",
    "        \n",
    "        # 2. Clean Response for ROUGE (Classification Only)\n",
    "        if \"Chain-of-Thought\" in name:\n",
    "            # For COT, look for the final classification on the last non-empty line\n",
    "            lines = [line.strip() for line in candidate_response.split('\\n') if line.strip()]\n",
    "            clean_response = lines[-1].replace(\"Classification:\", \"\").replace(\"Classification\", \"\").strip() if lines else candidate_response\n",
    "        else:\n",
    "            # For Zero/Few-Shot, clean the whole response\n",
    "            clean_response = candidate_response.replace(\"Classification:\", \"\").strip()\n",
    "        \n",
    "        # Strip potential markdown bolding characters\n",
    "        clean_response = clean_response.strip('*')\n",
    "\n",
    "        # 3. EVALUATION\n",
    "        rouge_l_f1 = calculate_rouge_l(clean_response, REFERENCE_ANSWER)\n",
    "        # Use the FULL response for PPL to evaluate overall fluency/coherence\n",
    "        perplexity_score = calculate_perplexity(candidate_response) \n",
    "        \n",
    "        # 4. Print Results\n",
    "        print(f\"\\n{'*'*70}\")\n",
    "        print(f\"EVALUATION RESULTS for {name} (Model: {MODEL_NAME}):\")\n",
    "        print(f\"  -> Reference Answer (Classification): {REFERENCE_ANSWER[0]}\")\n",
    "        print(f\"  -> Model's Cleaned Classification:    {clean_response}\")\n",
    "        print(f\"  -> ROUGE-L F1 Score:                {rouge_l_f1:.4f} (Measures Content Overlap, Max 1.0000)\")\n",
    "        print(f\"  -> Perplexity (PPL) Score:          {perplexity_score:.2f} (Measures Fluency/Confidence, Lower is Better)\")\n",
    "        print(f\"{'*'*70}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch API errors (e.g., authentication, rate limits)\n",
    "        print(f\"An API or execution error occurred (Error Type: {type(e).__name__}): {e}\")\n",
    "        print(\"Skipping evaluation for this prompt.\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3149a1ff-3213-4b82-9ab5-91af0ab85c41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
